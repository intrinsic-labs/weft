Lex Fridman & Tim Sweeney on developing the Verse programming language

Verse programming language
Lex Fridman
(02:52:00) So first of all, that’s a super exciting future where it’s not hundreds or thousands, it’s millions of creators that can just create different, small, or big elements of a world as big as earth. Just if you sort of close your eyes and imagine that world, that’s really exciting, where it’s not a centralized company controlling the release of a particular island or so on, it’s people constantly dynamically modifying all the islands of reality in this digital world.
(02:52:37) So if you could speak to some of the technologies that can enable that. You mentioned the Verse programming language. First of all, also, how legit is it for you, CEO of Epic Games to be a co-author? The programming language theorists are losing their mind. So a co-author on a paper that’s describing some of the nuanced details of a programming language. So maybe you could speak to this programming language called Verse. It’s a functional logic language. What are some cool features of Verse?
Tim Sweeney
(02:53:13) Verse is a programming language that we’re building for large scale simulation programming. It’s designed to make it easy to write code that can scale up to not only you building a Fortnite island, but you building modules or components that can be used by millions of other programmers and co-exist in a huge environment, and also can scale up to a huge scale simulation. Some games will be small. Battle Royale might find that a hundred players is actually optimal. It might be that a thousand player version of Battle Royale would be worse, but I bet there are a thousand, million, and tens of million player experiences there are even better than that, that will yet to be discovered.
Lex Fridman
(02:53:57) Wait a minute, tens of millions of players together?
Tim Sweeney
(02:54:03) Sure, we’ve had Fortnite events that have attracted 15 million concurrent users, but the fact that they’re all divided up into servers with a hundred players each for those events isn’t really a positive. It’s just a limitation of the technology.
(02:54:18) Tracing back to Unreal Engine 1 and its single threading decisions, if we could build a concert where all the concert participants, potentially tens of millions of them, could participate together simultaneously and see that there’s that massive a crowd, and they could all do interesting things and interact with each other, that would be way cooler.
Lex Fridman
(02:54:36) Sorry, I’m just loading it in, just imagining together in one scene graph, 10 million people interacting. What a cool world that is.
Tim Sweeney
(02:54:49) Sure. Well, 10 million people, you have less than 10 million pixels on your screen, so what does the Nyquist Sampling Theorem say? It says that you don’t need full overhead for every player. You need to render the players who are around you and some approximation of everything else.
Lex Fridman
(02:55:01) Yeah, but there’s also a networking component. You’re speaking to the rendering, but oh boy.
Tim Sweeney
(02:55:10) There’s a lot of work that has to happen there, but this is what we do for a living. We solve hard problems.
Lex Fridman
(02:55:12) I understand.
Tim Sweeney
(02:55:13) because if they’re easy then other people could have solved them already.
Lex Fridman
(02:55:16) That’s really cool though. Just sort of the possibility, the vision of that is really cool.
(02:55:21) Even a hundred thousand people or like 10,000 together just, I mean, there’s a reason in the physical world when you go to a concert and you have all those people around you, that energy, or you go to a football game, that energy is unlike anything else. And if you can bring that energy to the digital world, that’s amazing.
(02:55:43) But anyway, sorry, what, on the technology side of bringing that to life on the programming language side, can you continue, as I rudely interrupt you, talking about Verse?
Tim Sweeney
(02:55:55) Verse is a functional logic language because we think that that’s the way to make the most simple and powerful language simultaneously. Back in the 1970s, the programming language designer who built Pascal, one of the early programming languages, Niklaus Wirth or Nicholas Wirth as Americans might call him, stated this principle that programming language should achieve a high degree of power, not by having a lot of features, but by having a small number of features that work together and can be composed together arbitrarily so that you have to learn a relatively small set of things and then the real knowledge comes as you learn ways to combine them to achieve bigger and bigger programs.
(02:56:41) And so there’s a long history to the field of programming languages, but in the 1950s, the first programming language designers got together and built the first standardized language called ALGOL. And there was this meeting in 1956, very few people even know about it, but it’s where all the major foundations of modern programming languages were decided on, that the C family of languages inherited. And so we’re very much living in a world that was defined by them. And thankfully they got a whole lot of things right. They defined how functions should work, how variables should work, and how recursion should work. Thank God they got those things right, but they got a few things wrong. And Verse is trying to fix those. And that’s the functional logic part of it.
(02:57:22) The interesting thing about functional logic languages is that in an old school language, an expression produces a value. In a functional logic language, an expression can produce zero, one, or multiple values. And if it produces zero values, we might say it fails, and if it produces one value, we say it succeeds. And if it produces multiple values, it’s providing a set of values you could iterate over.
(02:57:45) And so there are a bunch of features in today’s programming languages that were defined in an ad hoc way without really thinking this through, this zero, one, or many values way. And that’s the problem that functional logic languages address. And the most basic example is an if statement in a programming language. If some condition holds, then do this thing, otherwise do that thing. And in a language today, this is done with variables of type boolean or expressions that produce booleans. We have boolean variables that are either true or false. We have expressions that evaluate to booleans. And so you can express a condition as a bunch of these features together, but you’ve lost any computation you’ve done in doing that boolean expression evaluation.
(02:58:28) So in a functional logic language, your condition wouldn’t do that. It would either succeed and produce a value or it would fail. If it succeeds, it goes to the then branch. Your operation succeeded, now you’re running this one batch of code. And if your expression failed, then you go to the else branch. But the exciting thing about that is your expression that succeeds or fails can produce values and bind variables that are then accessed by the then branch. So you can write a conditional where you can only get to the inside of the condition, to the then, if a bunch of variables have successfully been bound to variables. So it lets you test if some conditions hold and then use the results of those tests. And that gives you a much higher level of reliability.
(02:59:11) And then a for loop in a traditional language, it’s just a bunch of imperative code that’s woven together to produce a bunch of values iteratively. It’s rather awkward to do complicated things in for loops. And so you often end up with either ever more complicated constructs built to work around that like iterators and other things. The idea of functional logic languages is your for loop can just produce multiple values. And if it produces zero values, you’ve got to reiterate zero iterations, and it produces a bunch of values you’ve got to go through all of those as your iterations.
(02:59:41) Rather than having a bunch of nested loops, you can write arbitrary things that look like SQL queries in a condition or in a for loop that bind a bunch of variables, do a bunch of tests, produce a series of results in some order that you’re iterating over, and then you can handle all of them and produce a result. So you gain the power of SQL queries, large complex queries that are data structures in a language that is much simpler in which your code is just performing simple iterative operations. And so it gives you the best of databases and of regular programming in a much more uniform way. And the power of this is now users can write functions that not only produce a value, you can write functions that might fail. And so you can write a function that answers a question. The answer can be either yes and my value is this, or no. And you can combine these together into arbitrary queries.
(03:00:35) And I feel like the funny thing is that this is not how C++ works. And so when we have Epic programmers moving over from C++ and writing their first Verse code, they try to write C++ code in Verse style and it actually ends up being convoluted code that’s worse than good C++ or good Verse. When we see… But after a few months they get up to speed and they’re writing really awesome code that’s tighter and more compact than before. And with users who’ve never programmed before, but are learning programming for the first time in the context of Fortnite, it’s really fascinating. You see these users are learning this as, it becomes their intuition. They just assume programming works this way. And they’re writing way more advanced and interesting for loops and conditions than we’re often writing internally because they’ve grokked the core concepts.
Lex Fridman
(03:01:21) Yeah, you said a lot of really interesting stuff. First of all, it’s very interesting that there’s a bunch of people, a lot of people learning programming for the first time with Verse, which is a very different way to look at programming. And in some deep sense, as you’re saying, a very intuitive way to learn programming.
(03:01:41) But there’s a lot of properties about this being a logical language, one of which, we’ll maybe speak also about confluence, but also correctness. So being able to prove the correctness of a code, it’s basically easier to write bug free code.
(03:02:06) Can you just speak to that and the importance of that when you’re building the metaverse?
Tim Sweeney
(03:02:12) Yeah, right. So the challenge with the metaverse is first of all that it’s a huge base of code that’s evolving over time and written by many authors. So you might see every second a new module is updated somewhere. And you expect in this live ever running simulation that never shuts down for everything to upgrade live in place. And so, one critical component that is the ability to release an update to something you’ve already published and be sure that it’s backwards compatible with the one that you’ve already released. And that’s essentially a type checking problem, checking that your new interface is backwards compatible with your old one. And that comes down to the type system of the language. There’s been a lot of very interesting research on type systems over the years, most of which hasn’t ever made it into the C++ programming language unfortunately.
(03:03:01) But you see several branches of that whole field, one of the really interesting things that Java and C# did in the early days, and then later abandoned and didn’t bother update, was defining a very rigorous set of rules for if you publish a module with one set of types today, then what changes can you make to that module for your future updates to it that don’t break backwards compatibility? And that’s a problem for type checking. Like, say you have a function that promises to return some integer, well, in the future you could say that returns some natural number because every natural number is an integer. So that’s a backwards compatible change, but you can’t say it returns a rational number because some rational numbers are not integers. So the system ought to reject that kind of change.
(03:03:43) But the much, much, much more interesting thing about type checking, it was the realization, it was actually made in the 1930s that if you design a programming language type system in a very particular way, then it becomes not only useful for expressing types of variables, the traditional thing every type system does is say like, “Variable X is of type integer.” But if you design a type system in a certain way, then your types can express theorems, like mathematical theorems. The Pythagorean theorem is a cool one, but one theorem you might have in a program is like the theorem that this function takes an array of integers and returns an array of the same integers, but the result is sorted. If you express that as a theorem and you follow this system of type theory, then you can actually require that anybody who writes that sorting function to prove that it has actually sorted its result. And so you have types or theorems, and values constructed a certain way can be proofs of those theorems.
(03:04:46) And nowadays in mathematical literature, you see more and more theorems are being proven mechanically. Mathematicians are proving theorems in a way that is verified by computer to be a correct proof. In the old days of math, people would write down like language. If you look at all of Euclid’s theorems, it was just language. It was just writing in ancient Greek to say the steps of the proof to convince the reader that the thing is true.
(03:05:08) Starting in the 1930s, mathematicians moved towards rigorous formal proofs in which there is a series of steps that can be mechanically verified, that they’re proving things. And when mathematicians say they’ve done a computer proof of a theorem, what they really mean is they’ve written a program in a proof language, like Lean is a theorem prover, Coq is a theorem prover, and there are several others. It means they’ve written a mechanical proof in that language that a computer has checked so that it’s impossible to lie. If you say that you’ve proven a thing and the computer verifies it, then it’s definitely true.
(03:05:44) And this is a feature of mathematical proof languages, but it’s also an idea that’s making its way into programming languages gradually over time. And our aim for Verse is to be the first mainstream programming language that fully adopts that approach and that technique. And not only adopts it but adopts it in a way that’s really user-friendly, so you don’t have to do that. And the idea of this is that you want gradually more information to be incorporated in the types of variables. The property you want of a programming language is that if your compiler accepts your program, and doesn’t beep and tell you there is an error, then your program should work. Now there are all kinds of ways humans can make mistakes there so that we’ll never achieve that ideal, but we can get closer and closer to it by having more and more language features that enable the compiler to catch more human coding errors and tell the user what went wrong.
(03:06:37) And that becomes extremely important in the metaverse, the cost of fixing a bug that’s made it through to runtime and is in users’ hands, the cost of fixing a bug in a shipping program is hundreds of times higher than fixing a bug that you’ve just observed as you’re running your code yourself. When it’s running on your computer, you just fix a line of code and your bug’s fixed. When you have to fix it live, you have to release a patch, you have to release patch notes, you have to test the patch, you have to check for all the other bugs that might have been introduced, and everything becomes vastly, vastly more expensive. So the real aim of the Verse program and approach is to catch all of these errors at compile time and make the metaverse a very reliable place.
Lex Fridman
(03:07:18) Do you see a world where like at compile time you could prove that the program is correct in some sense of correctness?
Tim Sweeney
(03:07:25) Proving things becomes combinatorially harder as they get larger, right?
Lex Fridman
(03:07:29) Right.
Tim Sweeney
(03:07:29) And so the really important thing about this whole field is that you should be able to adopt these capabilities gradually and apply it where you really need it. Like, if you’re writing something like a cryptography algorithm, that’s a good place to prove stuff. If you’re writing a data decompressor that’s going to be used by an entire ecosystem, like proving that doesn’t overrun memory is actually really important. And a lot of the reason that security vulnerabilities happen today is because in a different language a compiler could have caught, were not caught in C because it just doesn’t have this feature.
(03:08:04) But we shouldn’t see this as scary. Everybody working in a typed language like C, or C#, or Java is proving theorems all the time. If you have a variable of type integer and you assign some value to it, you’ve proven to the compiler that that value was an integer because otherwise it would have rejected it. And so as we add more and more advanced proofs, we’ll get compositional properties flying out of our systems that they’re easy to use and people prefer to use.
(03:08:32) If we might think in the future where we have AI helping us write certain kinds of code, the big problem with AI is you ask it to do something, ask to write a fragment of code that does something, it might give you a perfectly valid fragment of code that compiles but does the wrong thing. And if we had languages where you could say, “Write a function that sorts this array and prove that it did that,” it could actually write the proof. And if the compiler didn’t beep with it, you could trust that it was actually sorting the array. And otherwise you could go back to the AI and say, “Well, that didn’t work.” But getting to the point where we know that our programs do what we say they’re going to do or think they’re going to do is a very important thing.
Lex Fridman
(03:09:12) And by the way, I should mention that you sent me a note about Curry-Howard correspondence, which I went down a rabbit hole, and that’s a whole fascinating field which shows the mathematical relationship between programs and proofs.
Tim Sweeney
(03:09:25) That’s right. This is a result from the 1930s. It’s one of the most important results of computer science that almost nobody knows about, but they did this rigorous breakdown of type systems and the 1930s formulation of programming, and established that everything you can prove in mathematical logic you can prove within a type system if it has certain features.
(03:09:51) And if you break down what is a proof? Well, a proof that integers exist is some integer, like five is a proof that integers exist. So when you have something like var X int, and you say “X = 5”, well you’re proving to the compiler that 5 is an integer. That comes as a secondhand nature, but you can prove more advanced things. If you want to prove that a pair of things are true, like theorem A is true and theorem B is true, then you need to provide a pair of values, one that proves theorem A and one that proves theorem B, and that’s the conjunctive law of proofs. And there’s a disjunctive law too.
(03:10:23) And then there’s an implication law for proofs. And it turns out that that’s really satisfied by functions. When you write a function in programming language, you’re saying, “If you give me this thing, I will give you that thing.” If you’ve given me a parameter of type something, then I’ll give you a result of some other type. And if you write that, by writing that function, you’re proving that given one of these things, you can produce another thing. And that’s a proof of an implication. With only like seven laws, you can construct all of mathematical logic in a type system.
(03:10:54) And one of the important things for programming languages that hasn’t been given enough attention is some aspects of programming languages are just subjective. They’re just machinations of the programming language designer. And Guido van Rossum decided that Python should support indentation in a certain way. And as long as you’re dealing with things like human notation and naming of things, there’s always going to be that subjective layer.
(03:11:18) But there are other parts of programming languages that are not subjective but should be fundamental. And when you look at type systems, there is a way to do type systems that gives you mathematical proofs. And every other way of type systems that doesn’t give you mathematical proofs is just worse and should ultimately be rejected.
(03:11:38) And so I think one of the jobs of computing is to identify what we actually done right in the past and what have we done wrong. And for everything we’ve done wrong, actually going back and fixing it. Otherwise, we just keep accumulating so much cruft that our systems eventually are crushed under their own complexity.
(03:11:54) And there have been massive announcements of horrible vulnerabilities in software and services over the past year. It turns out some nation-state backdoored a bunch of TELECO’s surveillance systems for wiretaps, like huge problem there. But ultimately when you break it down, it’s probably because of some buffer overrun in some C program. These decisions about programming languages have long-term implications.
Lex Fridman
(03:12:21) It’s really fascinating that in building these systems that hundreds of millions of people use, you’re rethinking about how do you actually build it from first principles.
(03:12:29) So I should mention that Verse’s primary design goal is it should be simple enough to learn as a first-time programmer, general enough for writing any kind of code and data, productive in the context of building, iterating, and shipping a project in a team setting, statically verified to catch as many categories of runtime problems as possible, compile time as we were talking about, performant for real-time open-world multiplayer games. We didn’t really quite talk about performance. Maybe I could ask you about that in a second. Complete so that every feature of the language supports programmer abstraction over that feature, timeless, built for the needs of today and for foreseeable future needs.
(03:13:08) And then there’s some design goals that we talked about, that is strongly typed. Multi-paradigm to use the best of functional programming, object-oriented programming, imperative programming so it’s as deterministic as possible. If you run it over and over, it runs in the exact same way. Failable expressions, as you talked about. It’s super fascinating, there’s so many cool features in this. Speculative execution, concurrency.
Concurrency
(03:13:33) Maybe can you talk about concurrency? What is it about Verse that allows for concurrency at the scale that you need?
Tim Sweeney
(03:13:41) This is the one biggest technical problem that we’re working to solve in this generation, and that is taming concurrency so that any ordinary programmer can achieve it by just writing ordinary code.
Lex Fridman
(03:13:57) It’s hard, yeah.
Tim Sweeney
(03:13:59) Programming on a single-threaded computer is hard enough, but it is completely predictable. If you have a language that’s deterministic and you’re on the same code, over and over, it’s always going to do exactly the same thing and there’s no unpredictability about what might happen, right? You’re reading and writing variables in some order and you’re always going to see it behave the same.
(03:14:19) The problem is when you introduce multiple threads or multiple nodes in a data center, all working together on a single problem, is that they each want to read and write different pieces of data, and change of the state of the world as they go. And still almost all concurrency in real-world programs today is achieved manually. Programmers are writing this code that might run in multiple threads very, very carefully so that they are negotiating among each thread to get access to data in a way that’s going to give them predictable results.
(03:14:53) And it’s incredibly hard. It’s so hard that we’ve in five generations of Unreal Engine, every single generation decided we’re not going to try to scale up all of our gameplay code to multiple threads manually. It’s just much, much, much too likely to go wrong, not only for ourselves, but for every partner company who licenses Unreal Engine and tries to use it for building a game. It’s just a massive foot gun.
(03:15:17) There’s a variety of solutions to concurrency that are all rather suboptimal. One attempted solution was like, just don’t try to solve this problem at all. Let’s break our program down into microservices. And almost all online websites of massive scale like amazon.com work with hundreds of microservices where different servers negotiate with each other by sending messages to each other. And by programmers writing those things very carefully, they eventually get to being able to take your orders and not make a mess of them reliably.
(03:15:48) But this is totally not scalable to the metaverse where you have millions of programmers who are mostly not going to be computer scientists. They’re mostly going to be hobbyists, and enthusiasts, and first time programmers doing stuff for fun. That’s never going to work for them because they’ll never be able to envision all of the different dependencies between different computations they’re running in parallel.
(03:16:07) But it turns out that there was an amazing foundational work done in 1980s that was made very real by a paper on Haskell concurrency. Composable Memory Transactions is the name of the paper. And it describes the system for transactional updates to programs. And the idea of a transaction is a transaction is a block of code that does a bunch of operations on memory, it might read, it might write, it might process an order, it might accept an order or reject an order. It might transfer money between one bank account and another. It might make conditional decisions like, “Oh, you asked to transfer a hundred dollars from your account to this guy’s account. We’re going to see if you have a hundred dollars. If you don’t, we’re going to reject it. And if you have a hundred dollars, we’re going to take a hundred dollars out of your account and add it to this other guy’s account.”
(03:16:56) Without transactions, if everybody’s just randomly adding and subtracting each other’s bank balances, then you might have somebody read a bank balance, subtract a hundred dollars and write it out. But in the meantime, somebody has written something else in the meantime. So you might get inconsistent bank balances arising if you don’t have a way of ensuring that these all run in a specific order.
(03:17:16) So the idea of transactions is its way of dividing an entire program into updates, self-contained updates that do an arbitrary amount of computation but must run in a single threaded manner. And in the case of a game engine, that’s a gameplay object update. When you’re playing Fortnite, you see a gameplay object. Every other player is a gameplay object. Every enemy is a gameplay object. Every rocket, and projectile, and car, and thing you see moving around and interacting, it’s not just a fixed static part of the world. That’s a separate game object. And each of those objects is updated at a rate of one update per frame at 60 frames per second.
(03:17:52) And so then in the course of Fortnite Battle Royale gameplay, you have tens of thousands of object updates happening every frame with a hundred players. In a simulation with billions of players, you’d have a whole lot more than that.
Lex Fridman
(03:18:04) So right now that’s done single-threaded?
Tim Sweeney
(03:18:05) Yeah, that’s done single-threadedly in each game session. This is why Fortnite has a hundred players limitation. If you absolutely maxed out a server, maybe today you could get it up to 140 or something, but it’s not going to thousands, or millions, or billions.
(03:18:18) And so what we need is a technique for magically automatically scaling our code to that. And transactions are the idea. And the idea is a transaction is a granule of code that runs its entirety. And so the idea of this transactional memory concept is that we’re going to have programmers write completely ordinary code that reads and writes variables in the completely ordinary way, and they’re not going to have to worry about concurrency at all. And then the system, like today, a program, a computer just runs your program. There’s no amount of speculation going on at the programming language level.
(03:18:50) The idea of transactions is since we have a bunch of operations we need to know we apply, we apply a large set of them concurrently, but instead of each one reading and writing from global memory shared by all, in which case they might be reading, and writing, and contending with each other for the same data, and might be doing contradictory things to it, we’re going to track all of our writes locally. We’re not going to write changes out to global memory. We’re going to keep track of it in a buffer that’s just for that one transaction. So it’s going to look to that code exactly as if it’s running on the global system affecting global game state, but it’s going to be isolated to just that one transaction. And it’s going to be set aside and buffered up for consideration later. We’re going to run tens, or hundreds, or thousands of the updates concurrently. We’re going to see which ones had read-write conflicts. Because if two transactions don’t read and write any of the same data, then you could have run them in either order or simultaneously, and it wouldn’t have changed the end result.
Lex Fridman
(03:19:51) Yeah, the order doesn’t matter. This is so fascinating. To imagine this kind of system arbitrarily concurrent running millions of updates in parallel of gameplay objects, that’s the thing that enables the thing that we’re talking about, which is tens of millions of people together in one scene.
Tim Sweeney
(03:20:13) Yeah, exactly. And the key is that you’re running these updates speculatively, and you’re not committing their changes to memory until you’re sure that they’re free of conflicts. So you might update 10,000 objects, you might find 9,000 of them were conflict-free. So you apply those 9,000 updates to memory, and they could have run in any order and it wouldn’t have changed the result.
Lex Fridman
(03:20:33) That’s so cool.
Tim Sweeney
(03:20:34) Now there’s a thousand objects left over. Now you have to run those again, try them, maybe interleave in a different way to get them to eventually commit to memory. And in the meantime, you just throw all of their computations away and redo them later.
(03:20:47) And by doing this, removing this from being a programming problem for the programmer to deal with, to being a language problem for us language designers to deal with. And we’re moving a vast amount of pain that would be imposed on a million people, instead to a vast amount of pain imposed on a small number of people to have to actually make this work.
Lex Fridman
(03:21:07) That’s amazing. That’s really incredible.